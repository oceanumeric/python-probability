{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit ('anaconda3': conda)",
   "metadata": {
    "interpreter": {
     "hash": "344443636c3027c5042750c9c609acdda283a9c43681b128a8c1053e7ad2aa7d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Geometric View of Probability Theory\n",
    "\n",
    "__Random variable (function)__: given an experiment with sample space $S$, a random variable function is a function from the sample space $S$ to the real numbers $R$. It is common, but not required, to denote random variables by capital letters. \n",
    "\n",
    "One way to think about probability is as a new number concept that allows you to tackle problems that have a special kind of uncertainty built into them. Thus, the key idea is that there is some number, say $x$, with a traveling companion, say, $f(x)$, and this companion represents the uncertainties about the value of x as if looking at the number x through a frosted window. The degree of opacity of the window is represented by $f(x)$. If we want to manipulate x, then we have to figure out what to do with f (x). For example if we want y = 2x, then we have to understand how $f(x)$ generates $f(y)$.\n",
    "\n",
    "Let us consider the following piecewise funtion:\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "1 & 0 < x \\leq 1 \\\\\n",
    "2 & 1 < x \\leq 2 \\\\\n",
    "0 & \\ \\text{otherwise} \\end{cases}$\n",
    "\n",
    "\n",
    "To compute the integral, we do:\n",
    "$$\\int_0^2 f d \\mu = 1 \\mu (\\{(0, 1]\\}) + 2 \\mu (\\{(1, 2]\\})$$\n",
    "\n",
    "By introducing the $\\mu$ function as a way of measuring the intervals above, we have introduced another degree of freedom in our integration. \n",
    "\n",
    "Unfortunately, the term random variable is not very descriptive. A better term is __measurable function__. The measurable function maps a set into a number on the real line. For example, $\\{1\\} \\to  1$ is one such function.\n",
    "\n",
    "Let's consider a slightly more interesting problem where we toss two dice. We assume that each throw is independent, meaning that the outcome of one does not influence the other. What are the sets in this case? They are all pairs of possible outcomes from two throws as shown below,\n",
    "$$\\Omega = \\{ (1, 1), (1, 2), \\cdots, (5, 6), (6, 6) \\}$$\n",
    "What are the measures of each of these sets?  By virtue of the independence claim, the measure of each is the product of the respective measures of each element. For instance,\n",
    "$$P((1, 2)) = P(\\{1\\}) P(\\{2\\}) = \\frac{1}{6^2}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sympy as S\n",
    "from sympy import stats, integrate, simplify\n",
    "from sympy.stats import density, E, Die\n",
    "from sympy.abc import y, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{2: 0.028, 3: 0.056, 4: 0.083, 5: 0.111, 6: 0.139, 7: 0.167, 8: 0.139, 9: 0.111, 10: 0.083, 11: 0.056, 12: 0.028}\n{False: 0.37, True: 0.63}\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sm\n",
       "2     0.018519\n",
       "3     0.037037\n",
       "4     0.055556\n",
       "5     0.092593\n",
       "6     0.129630\n",
       "7     0.166667\n",
       "8     0.148148\n",
       "9     0.129630\n",
       "10    0.111111\n",
       "11    0.074074\n",
       "12    0.037037\n",
       "Name: p, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# probability of sums of two dices\n",
    "d = {(i, j): i+j for i in range(1, 7) for j in range(1, 7)}\n",
    "dinv = defaultdict(list)\n",
    "for i, j in d.items():\n",
    "    dinv[j].append(i)\n",
    "X = {i:np.round(len(j)/36.0, 3) for i, j in dinv.items()}\n",
    "print(X)\n",
    "\n",
    "# probability that half the product of three dice will exceed their sum\n",
    "d = {(i, j, k):((i*j*k)/2 > i+j+k) for i in range(1, 7)\n",
    "                                   for j in range(1, 7)\n",
    "                                   for k in range(1, 7)}\n",
    "dinv = defaultdict(list)\n",
    "for i, j in d.items():\n",
    "    dinv[j].append(i)\n",
    "X={i:np.round(len(j)/6.0**3, 3) for i,j in dinv.items()}\n",
    "print(X)  # approximately 0.63\n",
    "\n",
    "# unfair dice\n",
    "df = pd.DataFrame(index=[(i, j) for i in range(1, 7) for j in range(1, 7)],\n",
    "            columns=['sm', 'd1', 'd2', 'pd1', 'pd2', 'p'])\n",
    "df.d1 = [i[0] for i in df.index]\n",
    "df.d2 = [i[1] for i in df.index]\n",
    "df.sm = list(map(sum, df.index))\n",
    "df.loc[df.d1<=3, 'pd1'] = 1/9\n",
    "df.loc[df.d1>3, 'pd1'] = 2/9\n",
    "df.pd2 = 1/6\n",
    "df.p = df.pd1 * df.pd2\n",
    "df.groupby('sm')['p'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "-log(3)/144 + 1/12\n-log(3)/144 + 1/12\n"
     ]
    }
   ],
   "source": [
    "fxy = x + y                 # joint density\n",
    "fy = integrate(fxy,(x,0,1)) # marginal density\n",
    "fx = integrate(fxy,(y,0,1)) # marginal density\n",
    "EXY = (3*y+2)/(6*y+3) # conditional expectation\n",
    "LHS=integrate((x-EXY)**2*fxy,(x,0,1),(y,0,1)) \n",
    "print(LHS) # left-hand-side\n",
    "# using Pythagorean theorem\n",
    "RHS=integrate((x)**2*fx,(x,0,1))-integrate((EXY)**2*fy,(y,0,1))\n",
    "RHS # right-hand-side\n",
    "print(RHS)"
   ]
  },
  {
   "source": [
    "Let $v = [1, 1]^T$ be the subspace onto which we want to _project_ $y$. At the closest point, the vector between $y$ and $x$ is perpendicular to the line. This means that\n",
    "$$(y-x)^T v = 0 $$,\n",
    "where $x$ is an arbitary point along the line:\n",
    "$$x = \\alpha v$$\n",
    "We can derive the projection operator:\n",
    "$$P_v = \\frac{v v^T}{||v||^2}$$\n",
    "Then, we can take any $y$ and find the closest point on $v$ by doing\n",
    "$$P_v y = v ( \\frac{v^T y}{||v||^2} ) $$\n",
    "\n",
    "With the understanding of projection, we now define the inner product for the random variables $X$ and $Y$ as\n",
    "$$<X, Y> = E(XY)$$\n",
    "So, when we have the __orthogonal projection__, we would have\n",
    "$$<X - h_{opt}(Y), Y> = 0$$\n",
    "\n",
    "> One could study Fourier Analysis to understand tht the entire theory of probability could be built upon the concept of orthgonality.\n",
    "\n",
    "__Conditional Expectation as Projection__. The conditional expectation is the minimum mean squared error (MMSE) solution to the following problem:\n",
    "$$\\min \\int_{R} (x - h(y))^2 d x$$\n",
    "with the minimizing $h_{opt}(y)$ as\n",
    "$$h_{opt}(Y) = E(X|Y)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Suppose we have two fair six-sided dice ($X$ and $Y$) and we want to measure the sum of the two variables as $Z = X + Y$ . Further, let’s suppose that given $Z$, we want the best estimate of $X$ in the mean-squared-sense. Thus, we want to minimize the following:\n",
    "$$J(\\alpha) = \\sum(x - \\alpha z)^2 P(x, y)$$\n",
    "We can substitute in for Z in J and get:\n",
    "$$J(\\alpha) = \\sum(x - \\alpha (x +y))^2 P(x, y)$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "329*a**2/6 - 329*a/6 + 91/6\n1/2\n"
     ]
    }
   ],
   "source": [
    "# conditional expectation as projection\n",
    "x = Die('D1', 6)  # 1st six sided die\n",
    "y = Die('D2', 6)\n",
    "a = S.symbols('a')\n",
    "z = x + y\n",
    "J = E((x-a*(x+y))**2)\n",
    "print(S.simplify(J))\n",
    "sol, = S.solve(S.diff(J, a), a)\n",
    "print(sol)"
   ]
  },
  {
   "source": [
    "\n",
    "Three coins, 10, 20 and 50p are tossed. The values of the coins that land heads up are totaled. What is the expected total given that two coins have landed heads up? In this case we have we want to compute $E(\\xi |\\eta)$ where\n",
    "$$\\xi := 10X_{10} + 20X_{20} + 50X_{50}$$\n",
    "where $X_i \\in \\{0, 1\\}$ and where $X_{10}$ is the Bernoulli-distributed random variable cor- responding to the 10p coin (and so on). Thus, $\\xi$ represents the total value of the heads-up coins. The $\\eta$ represents the condition that only two of the three coins are heads-up,\n",
    "$$\\eta := X_{10} X_{20}(1 − X_{50}) + (1 − X_{10})X_{20} X_{50} + X_{10}(1 − X_{20})X_{50}$$\n",
    "and is a function that is non-zero only when two of the three coins lands heads-up.\n",
    "\n",
    "To compute the conditional expectation, we want to find a function h of η that minimizes the mean-squared-error (MSE),\n",
    "$$MSE = \\sum_{X \\in [0, 1]^3} \\frac{1}{2^3}(\\xi - h(\\eta))^2$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "X10, X20, X50 = S.symbols('X10, X20, X50', real=True)\n",
    "xi = 10*X10 + 20*X20 + 50*X50\n",
    "eta = X10*X20*(1-X50) + X10*(1-X20)*(X50) + (1-X10)*X20*(X50)\n",
    "num = S.summation(xi*eta, (X10, 0, 1), (X20, 0, 1), (X50, 0, 1))\n",
    "den = S.summation(eta*eta, (X10, 0, 1), (X20, 0, 1), (X50, 0, 1))\n",
    "alpha = num/den\n",
    "print(alpha)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "160/3\n"
     ]
    }
   ]
  },
  {
   "source": [
    "The insights from the above analysis is that given the additional measuremnt, we work with the conditional or a _posterior density_ $f_{Y|X}(y|x)$, and the best estimation is the conditional expectation:\n",
    "$$\\hat{y}(x) = E[Y |X = x]$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### References\n",
    "\n",
    "Lecture Notes from [MIT](https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-011-introduction-to-communication-control-and-signal-processing-spring-2010/readings/MIT6_011S10_chap08.pdf)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
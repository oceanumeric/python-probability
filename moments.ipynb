{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('pyfinance')",
   "metadata": {
    "interpreter": {
     "hash": "2e9997a74c05648347465b7dbd84bcb5d10e7dab1256bcba193691ec12ee0217"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Moments\n",
    "\n",
    "In this notes, we explore how the moments of an random variable shed light on its distribution. Most people know the first two moments of a distribution - mean $E(X)$ and variance $E(X^2) - (EX)^2$, which are important summaries of the average value of X and how spread out its distribution is. But there is much more to a distribution than its mean and variance.\n",
    "\n",
    "Key Ideas:\n",
    "\n",
    "- moments generating functions are like bridge between sequence of numbers and the world of calculus.\n",
    "-  starting with a sequence of numbers, create a continuous function—the generating function—that encodes the sequence.\n",
    "\n",
    "The _moment generating function_ of an random variable $X$ is $M(t) = E(e^{tX})$, as a function of $t$, if this is finite on some open interval $(−a, a)$ containing $0$. Otherwise we say the MGF of $X$ does not exist. \n",
    "\n",
    "> A natural question at this point is “What is the interpretation of $t$?” The answer is that $t$ has no interpretation in particular; it’s just a bookkeeping device that we introduce in order to be able to use calculus instead of working with a discrete sequence of moments.\n",
    "\n",
    "The first moment is the mean, which we can easily compute from the definition as,\n",
    "$$\\frac{d M(t)}{dt} = \\frac{d}{dt} E(\\exp(tX)) = E \\frac{d}{dt}(\\exp(tX)) = E(X \\exp(tX))$$\n",
    "Now, we have to set $t=0$ and we have the mean,\n",
    "$M^{(1)}(0) = E(X)$\n",
    "continuing this derivative process again, we obtain the second moment as,\n",
    "\\begin{align}\n",
    "M^{(2)}(t) & = E(X^2 \\exp(t X))\\\\\n",
    "M^{(2)}(0) & = E(X^2)\n",
    "\\end{align}\n",
    "\n",
    "## Why are they called \"moments\"?\n",
    "\n",
    "First, watch this [video](https://youtu.be/lwO0V5FitAo), and make sure you understand the relationship between moment of inertia and distribution of points of masses. In probability, moments quantify three parameters of distributions: location, shape, and scale.\n",
    "\n",
    "Many distributions have parameters that are called \"location\", \"scale\", or \"shape\" because they control their respective attributes, but some do not. \n",
    "\n",
    "## Different distributions have different moments generating functions. \n",
    "\n",
    "For $X \\sim Geom(p)$, we have \n",
    "$$M(t) = E(e^{tX}) = \\sum_{k=0}^\\infty e^{tk}q^k p = p \\sum_{k=0}^\\infty (qe^t)^k = \\frac{p}{1-qe^t}$$\n",
    "\n",
    "The moments generating function of a standard normal random variable $X$ is\n",
    "$$M_X(t) = E(e^{tX}) = \\int_{-\\infty}^\\infty e^{tx} \\frac{1}{\\sqrt{2\\pi}} e^{-x^2/2} d x = e^{t^2/2}$$\n",
    "\n",
    "## Relationship between moments generating functions and moments\n",
    "\n",
    "The moment-generating function is so named because it can be used to find the moments of the distribution. The series expansion of $e^{tX}$ is\n",
    "$$e^{tX} = 1 + tX + \\frac{t^2X^2}{2!} + \\frac{t^3X^3}{3!} + \\cdots + \\frac{t^n X^n}{n!}$$\n",
    "Hence\n",
    "\\begin{align}\n",
    "M_X(t) & = E(e^{tX}) = 1 + t E(X) + \\frac{t^2 E(X^2)}{2!} + \\frac{t^3E(X^3)}{3!} + \\cdots + \\frac{t^nE(X^n)}{n!} \\\\\n",
    "& = 1 + t m_1 + \\frac{t^2m_2}{2!} + \\cdots,\n",
    "\\end{align}\n",
    "where $m_n$ is the $n$th moment. Differentiating $M_X(t)$ $i$ times with respect to $t$ and setting $t=0$, we obtain the $i$th moment about the origin, $m_i$. \n",
    "\n",
    "That's why we have the equation for the $k$th moment of a random variable $X$ with a density function $f(x)$,\n",
    "$$\\mu_k = E[X^k] = \\int_{-\\infty}^\\infty x^k f(x) dx$$\n",
    "\n",
    "So, $k$th moment of a random variable is just __the expectation of random variable $X^k$__.\n",
    "\n",
    "__Remark__: To gain the intution for momenets generating functions, one could always rely on the Taylor series expansion along point $0$. We want to have a function (our moments generating function) with the following properties:\n",
    "\n",
    "* reflect an infinite sum of weighted raw moments\n",
    "* encode a countable sequence of numbers, or events as all the measurements have to be countable (or discrete)\n",
    "* maps to a well-behaved function (such as continuous function or even have higher order derivative $C^k$)\n",
    "* Universe might be continuous, infinite, and uncountable, but our lives are discrete, finite and countable.\n",
    "* moments refers to how probability mass is distributed from the 'central point' (or initial moment $t=0$). \n",
    "* if $e^{tx}$ is the measurement in _time_ dimension, then moments could be regarded as the measurement in _space_ dimension. \n",
    "\n",
    "### From 0th moment to kth moment \n",
    "\n",
    "$x^0 = 1$ for any number $x$, the zeroth raw, central, and standardized moments are all\n",
    "$$\\mu_0 = E[X^0] = \\int_{-\\infty}^\\infty f(x) dx = 1$$,\n",
    "which is the just the integration of _probability density function_ $f(x)$. \n",
    "\n",
    "The first raw moment is \n",
    "$$\\mu_1 = E[X] = \\int_{-\\infty}^\\infty x f(x) dx$$\n",
    "\n",
    "The second raw moment is\n",
    "$$m_2 = E[X^2] = \\int_{-\\infty}^\\infty x^ f(x) dx$$\n",
    "\n",
    "However, we are more interested in the _second central moment_, rather than the second raw moment, this is called the variance:\n",
    "$$V[X] = \\int_{-\\infty}^\\infty (x- \\mu_x)^2 f(x) dx$$\n",
    "\n",
    "Formula for calculating variance based on the moments:\n",
    "$$V(X) = E(X^2) - (EX)^2$$\n",
    "\n",
    "### Law of the unconscious statistician (LOTUS)\n",
    "\n",
    "In probability theory and statistics, the law of the unconscious statistician (LOTUS) is a theorem used to calculate the expected value of a function $g(X)$ of a random variable $X$ when one knows the probability distribution of $X$ but one does not know the distribution of $g(X)$. We can do this because of the _unique theory of moment generating function_. Once we knw the moment of $g(x)$\n",
    "$$E[g(X)^k] = \\int_{\\infty}^\\infty g(x)^k f_X(x) dx$$,\n",
    "we can get the distribution. \n",
    "\n",
    "\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Summary\n",
    "\n",
    "<img src=\"images/moments.png\">\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as S\n",
    "from sympy import stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10*p\n10*p\n10*p*(9*p + 1)\n"
     ]
    }
   ],
   "source": [
    "p, t = S.symbols ('p t', positive=True)\n",
    "x = stats.Binomial('x', 10, p)  # binomal distribution\n",
    "mgf = stats.E(S.exp(t*x))\n",
    "print(S.simplify(stats.E(x)))\n",
    "print(S.simplify(stats.moment(x, 1)))  # mean\n",
    "print(S.simplify(stats.moment(x, 2)))  # second moment"
   ]
  },
  {
   "source": [
    "### References\n",
    "\n",
    "* Understanding Moments by [Gregory Gundersen](https://gregorygundersen.com/blog/2020/04/11/moments/)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}